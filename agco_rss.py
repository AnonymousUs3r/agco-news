import sys
import hashlib
import requests
from datetime import datetime, timezone
from bs4 import BeautifulSoup
from feedgen.feed import FeedGenerator

URL = "https://www.agco.ca/en/views/ajax"

PARAMS = {
    "_wrapper_format": "drupal_ajax",
    "keys": "",
    "field_news_type": "All",
    "field_line_of_business": "2091",  # Lottery and Gaming
    "sort_bef_combine": "field_published_date_DESC",
    "view_name": "search_news",
    "view_display_id": "block_1",
    "view_path": "/node/376811",
    "pager_element": 0,
    "_drupal_ajax": 1,
}

def scrape_agco():
    print("üåê Fetching filtered AGCO news via direct request...")
    try:
        response = requests.get(URL, params=PARAMS, timeout=15)
        response.raise_for_status()

        chunks = response.json()
        html_fragment = next(
            (chunk["data"] for chunk in chunks
             if isinstance(chunk, dict) and
                chunk.get("command") == "insert" and
                isinstance(chunk.get("data"), str) and
                "<div" in chunk["data"]),
            None
        )

        if not html_fragment:
            print("‚ùå Could not find insertable HTML content in AJAX response.")
            sys.exit(1)

        soup = BeautifulSoup(html_fragment, "html.parser")
        return soup

    except Exception as e:
        print(f"‚ùå Failed to fetch AGCO news: {e}")
        sys.exit(1)

def parse_feed(soup):
    items = soup.select("div.border-t-2")
    if not items:
        print("‚ö†Ô∏è No filtered news items found.")
        sys.exit(1)

    fg = FeedGenerator()
    fg.id("https://www.agco.ca/en/general/news")
    fg.title("AGCO News ‚Äì Lottery and Gaming")
    fg.link(href="https://www.agco.ca/en/general/news", rel="alternate")
    fg.description("Filtered AGCO Ontario news (Lottery and Gaming)")
    fg.language("en")

    for item in items:
        link_tag = item.select_one("h5 a")
        date_tag = item.select_one("time[datetime]")

        if not (link_tag and date_tag):
            continue

        title = link_tag.get_text(strip=True)
        href = link_tag["href"]
        full_link = "https://www.agco.ca" + href
        date_text = date_tag.get_text(strip=True)

        try:
            dt = datetime.strptime(date_text, "%B %d, %Y")
            pub_date = datetime(dt.year, dt.month, dt.day, 23, 59, 0, tzinfo=timezone.utc)
        except Exception as e:
            print(f"‚ö†Ô∏è Date parse issue for: {title} ‚Äî {e}")
            pub_date = datetime.now(timezone.utc)

        guid = hashlib.md5((title + full_link).encode("utf-8")).hexdigest()

        entry = fg.add_entry()
        entry.id(guid)
        entry.guid(guid, permalink=False)
        entry.title(title)
        entry.link(href=full_link)
        entry.pubDate(pub_date)
        entry.updated(pub_date)

        print(f"‚úÖ {title} ‚Äî {pub_date.date()}")

    fg.rss_file("agco_feed.xml")
    print("üìÑ Saved AGCO RSS to agco_feed.xml")

if __name__ == "__main__":
    soup = scrape_agco()
    parse_feed(soup)
